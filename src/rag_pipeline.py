from .llm_service import GeminiService
from .vector_db import QdrantService
from .graph_db import Neo4jService

class GraphRAG:
    def __init__(self):
        self.llm = GeminiService()
        self.vectordb = QdrantService()
        self.graphdb = Neo4jService()

    def query(self, user_question):
        print(f"\nğŸ” Äang phÃ¢n tÃ­ch cÃ¢u há»i: '{user_question}'...")
        
        # BÆ¯á»šC 1: TÃ¬m kiáº¿m Vector (Semantic Search)
        # TÃ¬m cÃ¡c phim cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng Ä‘á»“ng
        query_vec = self.llm.get_embedding(user_question, task_type="retrieval_query")
        
        if not query_vec:
            return "Xin lá»—i, há»‡ thá»‘ng Ä‘ang báº­n, khÃ´ng thá»ƒ táº¡o vector."

        search_results = self.vectordb.search(query_vec, top_k=4) # Láº¥y top 4
        
        if not search_results:
            # Náº¿u khÃ´ng tÃ¬m tháº¥y káº¿t quáº£ trong Vector DB, cho phÃ©p LLM dÃ¹ng kiáº¿n thá»©c chung
            # Ä‘á»ƒ Ä‘Æ°a ra gá»£i Ã½ thay vÃ¬ tráº£ vá» ngay má»™t thÃ´ng bÃ¡o lá»—i.
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ trong Vector DB â€” chuyá»ƒn sang LLM Ä‘á»ƒ gá»£i Ã½ dá»±a trÃªn kiáº¿n thá»©c chung.")
            answer = self.llm.generate_answer("", user_question, context_provided=False, ask_followups=True)
            return answer

        # Láº¥y ra danh sÃ¡ch ID phim tÃ¬m Ä‘Æ°á»£c (payload key: movie_id)
        found_ids = []
        for hit in search_results:
            payload = getattr(hit, 'payload', {})
            mid = payload.get('movie_id') or payload.get('tmdb_id') or payload.get('id')
            if mid:
                found_ids.append(mid)
        print(f"âœ… Vector DB tÃ¬m tháº¥y {len(found_ids)} phim tiá»m nÄƒng.")

        # BÆ¯á»šC 2: Truy váº¥n Graph (Context Enrichment)
        # DÃ¹ng ID Ä‘á»ƒ láº¥y thÃªm thÃ´ng tin cáº¥u trÃºc (Äáº¡o diá»…n, Diá»…n viÃªn, quan há»‡...)
        print("ğŸ•¸ï¸  Äang truy váº¥n Graph Database...")
        graph_context = self.graphdb.get_graph_context(found_ids)
        
        if not graph_context:
            graph_context = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin chi tiáº¿t trong Graph DB."

        # BÆ¯á»šC 3: Tá»•ng há»£p cÃ¢u tráº£ lá»i báº±ng LLM
        print("ğŸ¤– Äang tá»•ng há»£p cÃ¢u tráº£ lá»i...")
        context_provided = bool(graph_context and graph_context.strip())
        answer = self.llm.generate_answer(graph_context, user_question, context_provided=context_provided, ask_followups=True)
        return answer

    def close(self):
        self.graphdb.close()