# Manual RAGAS Evaluation - Usage Guide

## ğŸ“š Tá»•ng quan

Script Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng RAG system sá»­ dá»¥ng **Manual RAGAS metrics** vá»›i LLM-as-Judge approach. Há»— trá»£ nhiá»u datasets vÃ  metrics nÃ¢ng cao.

---

## ğŸ¯ TÃ­nh nÄƒng

### âœ… Metrics (7 total)
**Core Metrics (5):**
1. **Faithfulness** - KhÃ´ng hallucination
2. **Answer Relevancy** - Tráº£ lá»i Ä‘Ãºng cÃ¢u há»i
3. **Context Precision** - Retrieval cháº¥t lÆ°á»£ng cao
4. **Context Recall** - Retrieval Ä‘áº§y Ä‘á»§
5. **Answer Correctness** - ChÃ­nh xÃ¡c so vá»›i ground truth

**New Metrics (2):**
6. **Response Completeness** ğŸ†• - User satisfaction
7. **Source Attribution** ğŸ†• - Traceability

### âœ… Dataset Support
- Load tá»« folder `test_datasets/`
- Chá»n dataset cá»¥ thá»ƒ hoáº·c "all"
- Há»— trá»£ format chuáº©n vá»›i metadata

### âœ… Weighted Scoring
- Critical metrics cÃ³ trá»ng sá»‘ cao hÆ¡n
- Overall weighted score + simple average
- Transparent reasoning tá»« LLM

---

## ğŸ“ Dataset Structure

Folder `test_datasets/` chá»©a cÃ¡c datasets:
```
test_datasets/
â”œâ”€â”€ actor_based.json          # Queries vá» diá»…n viÃªn
â”œâ”€â”€ comparison.json            # So sÃ¡nh phim/Ä‘áº¡o diá»…n
â”œâ”€â”€ director_based.json        # Queries vá» Ä‘áº¡o diá»…n
â”œâ”€â”€ genre_recommendation.json  # Gá»£i Ã½ theo thá»ƒ loáº¡i
â”œâ”€â”€ multi_hop.json             # Multi-hop reasoning
â”œâ”€â”€ specific_film_info.json    # ThÃ´ng tin phim cá»¥ thá»ƒ
â””â”€â”€ temporal_based.json        # Queries vá» thá»i gian
```

**Dataset Format:**
```json
{
  "category": "actor_based",
  "description": "Queries about actors...",
  "test_cases": [
    {
      "id": 1,
      "query": "Which actors have worked with Nolan?",
      "entities": ["Christopher Nolan"],
      "relations": ["ACTED_IN", "DIRECTED_BY"],
      "complexity": "high",
      "ground_truth": "..." // Optional
    }
  ]
}
```

---

## ğŸš€ Usage

### 1. List Available Datasets
```bash
python manual_ragas_evaluation.py
```

**Output:**
```
ğŸ“š Available datasets:
   1. actor_based.json
   2. comparison.json
   3. director_based.json
   ...
```

---

### 2. Evaluate Single Dataset

#### Táº¥t cáº£ queries trong dataset
```bash
python manual_ragas_evaluation.py --dataset actor_based.json
```

#### Giá»›i háº¡n sá»‘ lÆ°á»£ng queries
```bash
python manual_ragas_evaluation.py --dataset actor_based.json --num 5
```

#### Short form
```bash
python manual_ragas_evaluation.py -d comparison.json -n 3
```

---

### 3. Evaluate ALL Datasets

#### All queries from all datasets
```bash
python manual_ragas_evaluation.py --dataset all
```

#### First N queries from each dataset
```bash
python manual_ragas_evaluation.py --dataset all --num 3
```

**Output:**
```
ğŸ“š Loading ALL datasets (7 total):
   âœ“ actor_based.json: 3 queries
   âœ“ comparison.json: 3 queries
   âœ“ director_based.json: 3 queries
   ...

âœ… Total loaded: 21 queries
```

---

### 4. Custom Dataset Directory
```bash
python manual_ragas_evaluation.py \
  --dataset actor_based.json \
  --datasets-dir my_custom_datasets/
```

---

## ğŸ“Š Execution Flow

### Step 1: Configuration
```
ğŸ”¬ Manual RAGAS Evaluation (Enhanced)
GraphRAG vs SimpleRAG Comparison with LLM-as-Judge
================================================================================

ğŸ“Š Evaluation Plan:
   â€¢ Total queries: 15
   â€¢ Categories:
      - actor_based: 5 queries
      - comparison: 10 queries
   â€¢ Metrics: 7 (5 core + 2 new)
   â€¢ Estimated time: ~30 minutes

âš ï¸  This will evaluate 15 queries. Continue? (y/n):
```

### Step 2: System Initialization
```
ğŸš€ Initializing RAG systems...
   Connecting to Qdrant...
   Connecting to Neo4j...
   Loading embedding model...
âœ“ Systems ready
```

### Step 3: Query Evaluation
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query 1/15
Category: actor_based
Complexity: high
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[Query 1] Which actors have worked with Christopher Nolan multiple times?
Category: actor_based

  ğŸ”· GraphRAG:
    â†’ Evaluating: Which actors have worked with Christopher Nola...

    ğŸ“ FULL ANSWER:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Christian Bale, Michael Caine, Tom Hardy, and Cillian Murphy have all 
    collaborated with Christopher Nolan on multiple films...
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    ğŸ“š CONTEXTS (3 total):
       1. Title: The Dark Knight | Director: Christopher Nolan...
       2. Title: Inception | Director: Christopher Nolan...
       3. Title: Interstellar | Director: Christopher Nolan...

      â€¢ Faithfulness... 0.950
      â€¢ Answer Relevancy... 0.920
      â€¢ Context Precision... 0.867
      â€¢ Context Recall... 0.800
      â€¢ Answer Correctness... 0.880
      â€¢ Response Completeness (NEW)... 0.900
      â€¢ Source Attribution (NEW)... 0.750

      â­ Overall Score (weighted): 0.882
      â­ Overall Score (simple): 0.867

  ğŸ”¶ SimpleRAG:
    [Similar output...]

  ğŸ“Š Comparison:
    GraphRAG Overall (weighted): 0.882
    SimpleRAG Overall (weighted): 0.756
    GraphRAG Overall (simple): 0.867
    SimpleRAG Overall (simple): 0.743
    ğŸ† Winner: GraphRAG

â³ Progress: 1/15 completed (6.7%)
```

### Step 4: Final Report
```
================================================================================
ğŸ“ˆ GENERATING FINAL REPORT
================================================================================

================================================================================
ğŸ”¬ MANUAL RAGAS EVALUATION REPORT (Enhanced v2.0)
================================================================================

Metric                         GraphRAG        SimpleRAG       Improvement    
--------------------------------------------------------------------------------
ğŸ“Š Core Metrics:
  faithfulness                 0.9240          0.8120          +13.79%
  answer_relevancy             0.9010          0.8450          +6.63%
  context_precision            0.8650          0.7230          +19.64%
  context_recall               0.8320          0.7890          +5.45%
  answer_correctness           0.8980          0.8210          +9.38%

ğŸ†• New Metrics:
  response_completeness        0.8870          0.7980          +11.15%
  source_attribution           0.7650          0.6420          +19.16%

â­ Overall Scores:
  overall_weighted             0.8821          0.7858          +12.25%
  overall_simple               0.8674          0.7757          +11.82%

âœ“ Report saved to: manual_ragas_report_actor_based_20260106_153042.json

âœ… Evaluation complete!
ğŸ“ Report saved: manual_ragas_report_actor_based_20260106_153042.json
ğŸ“Š Queries evaluated: 15

ğŸ“ˆ Quick Summary:
   â€¢ GraphRAG: 0.882
   â€¢ SimpleRAG: 0.786
   â€¢ Winner: GraphRAG ğŸ†

================================================================================
```

---

## ğŸ“„ Output Files

### Report Format
```json
{
  "metadata": {
    "evaluation_date": "2026-01-06 15:30:42",
    "total_queries": 15,
    "method": "Manual RAGAS Implementation (LLM-as-Judge) v2.0 Enhanced",
    "metrics_count": 7,
    "new_metrics": ["response_completeness", "source_attribution"]
  },
  "graphrag_metrics": {
    "faithfulness": 0.924,
    "answer_relevancy": 0.901,
    "context_precision": 0.865,
    "context_recall": 0.832,
    "answer_correctness": 0.898,
    "response_completeness": 0.887,
    "source_attribution": 0.765,
    "overall_weighted": 0.882,
    "overall_simple": 0.867
  },
  "simplerag_metrics": { ... },
  "improvements": {
    "faithfulness": 13.79,
    "answer_relevancy": 6.63,
    ...
  },
  "detailed_results": [
    {
      "query_id": 1,
      "question": "Which actors...",
      "category": "actor_based",
      "complexity": "high",
      "graphrag": {
        "answer": "...",
        "contexts_count": 5,
        "metrics": { ... }
      },
      "simplerag": { ... },
      "winner": "GraphRAG"
    }
  ]
}
```

### File Naming
```
manual_ragas_report_{dataset}_{timestamp}.json

Examples:
- manual_ragas_report_actor_based_20260106_153042.json
- manual_ragas_report_all_20260106_160530.json
- manual_ragas_report_comparison_20260106_143210.json
```

---

## âš™ï¸ Configuration

### Debug Mode
Hiá»ƒn thá»‹ LLM reasoning:
```python
# In code:
evaluator.debug_mode = True   # Show detailed reasoning
evaluator.debug_mode = False  # Clean output only
```

### Retry Settings
```python
# In _call_llm_with_retry():
max_retries = 3  # Number of retries on API failure
```

### Metric Weights
```python
# In evaluate_single():
weights = {
    'faithfulness': 1.5,          # Critical
    'answer_relevancy': 1.5,      # Critical
    'answer_correctness': 1.5,    # Critical
    'context_precision': 1.0,     # Important
    'context_recall': 1.0,        # Important
    'response_completeness': 0.8, # Nice to have
    'source_attribution': 0.7     # Nice to have
}
```

---

## ğŸ“ˆ Performance

| Aspect | Value |
|--------|-------|
| **Time per query** | ~2 minutes |
| **LLM calls per query** | 14 (7 metrics Ã— 2 systems) |
| **Token usage per query** | ~4,000-6,000 tokens |
| **Rate limiting** | 2s between queries |

**Estimated times:**
- 5 queries: ~10 minutes
- 10 queries: ~20 minutes
- 20 queries: ~40 minutes
- All datasets (~50 queries): ~100 minutes

---

## ğŸ› ï¸ Troubleshooting

### Issue: "No datasets found"
**Solution:**
```bash
# Check directory exists
ls test_datasets/

# Specify custom directory
python manual_ragas_evaluation.py --datasets-dir path/to/datasets/
```

### Issue: API rate limits
**Solution:**
```python
# Increase delay in main():
time.sleep(3)  # Instead of 2 seconds
```

### Issue: Evaluation hangs
**Solution:**
- Check LLM API key valid
- Check timeout settings (default 30s)
- Enable debug mode to see where it hangs

### Issue: Low scores across the board
**Possible causes:**
- Poor retrieval quality
- Incomplete contexts
- Check individual metric scores to diagnose

---

## ğŸ’¡ Best Practices

1. **Start Small**: Test with `--num 3` first
2. **Debug Mode**: Enable for first run to understand scoring
3. **Ground Truth**: Provide when available for better accuracy
4. **Dataset Selection**: Start with specific datasets before "all"
5. **Save Reports**: Keep reports for comparison over time

---

## ğŸ“š Examples

### Quick Test (3 queries)
```bash
python manual_ragas_evaluation.py -d actor_based.json -n 3
```

### Full Dataset
```bash
python manual_ragas_evaluation.py -d comparison.json
```

### All Datasets (Sample)
```bash
python manual_ragas_evaluation.py --dataset all --num 2
```

### Production Evaluation
```bash
# All queries from specific categories
python manual_ragas_evaluation.py -d multi_hop.json > eval_multihop.log 2>&1
```

---

## ğŸ”— Related Files

- `manual_ragas_evaluation.py` - Main script
- `RAGAS_IMPROVEMENTS.md` - Detailed improvements documentation
- `test_datasets/*.json` - Dataset files
- `manual_ragas_report_*.json` - Output reports

---

## ğŸ“ Notes

- **Cost**: ~$0.10-0.20 per 10 queries (Gemini API)
- **Accuracy**: LLM-based, scores may vary slightly between runs
- **Bias**: Weighted scores favor critical metrics (faithfulness, relevancy, correctness)

---

## ğŸš€ Future Enhancements

- [ ] Parallel query evaluation
- [ ] HTML report generation
- [ ] Confidence intervals for scores
- [ ] Category-specific weights
- [ ] Interactive mode for query selection

---

**Last Updated**: January 6, 2026  
**Version**: 2.0 (Enhanced with dataset support)
