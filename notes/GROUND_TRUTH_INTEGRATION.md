# Ground Truth Integration Summary

## Overview

Successfully integrated ground truth from `test_datasets/` and `test_dataset.json` into the MRR/MAP evaluation system. The system now automatically extracts relevant documents by matching query entities with movie metadata in the crawled database.

## Files Updated

### 1. **evaluate_mrr_map.py** - Main evaluation script
- ‚úÖ `load_movie_database()`: Loads 654 movies from crawled_data/movies_index.json
- ‚úÖ `load_test_queries_from_datasets()`: Parses all test_datasets/*.json files
- ‚úÖ Automatic relevance extraction via entity matching
- ‚úÖ Category breakdown for analysis
- ‚úÖ Fallback to manual ground truth if datasets unavailable

### 2. **test_ground_truth_integration.py** - Standalone test utility
- Quick validation without full RAG dependencies
- Shows ground truth statistics and quality metrics
- Sample query analysis

## Data Sources

### Test Datasets Loaded (35 total queries)

| Source | File | Queries | Status |
|--------|------|---------|--------|
| Actor-based | test_datasets/actor_based.json | 5 | ‚úÖ Loaded |
| Director-based | test_datasets/director_based.json | 5 | ‚úÖ Loaded |
| Multi-hop | test_datasets/multi_hop.json | 5 | ‚úÖ Loaded |
| Comparison | test_datasets/comparison.json | 5 | ‚úÖ Loaded |
| Temporal | test_datasets/temporal_based.json | 5 | ‚úÖ Loaded |
| Genre Recommendation | test_datasets/genre_recommendation.json | 0 | ‚ö†Ô∏è Empty |
| Film Info | test_datasets/specific_film_info.json | 0 | ‚ö†Ô∏è Empty |
| Main Dataset | test_dataset.json | 5 | ‚úÖ Loaded |

**Total: 25 test queries with automatic ground truth extraction**

## Ground Truth Quality

```
üìä GROUND TRUTH ANALYSIS
===========================
Total Queries:           25
Queries with Relevant:   15 (60%)
Total Relevant Docs:     96
Avg Relevant Per Query:  3.84

üìë Category Distribution:
  actor_filmography      : 5 queries
  director_filmography   : 5 queries
  multi_hop              : 5 queries
  comparison             : 5 queries
  temporal_based         : 5 queries
```

## Relevance Matching Strategy

The system matches query entities to relevant documents using:

1. **Title Matching**: Direct match with movie title
2. **Director Matching**: Entity matches director name
3. **Cast Matching**: Entity matches any actor/cast member
4. **Genre Matching**: Entity matches movie genre
5. **Overview Matching**: Entity appears in movie synopsis

### Example

**Query:** "Which actors have worked with Christopher Nolan multiple times?"
- **Entity:** "Christopher Nolan"
- **Matches:** All movies with Christopher Nolan as director
- **Relevant Docs:** [inception, the_dark_knight, interstellar, oppenheimer, ...]

## Usage

### Run Evaluation with Integrated Ground Truth

```bash
python evaluate_mrr_map.py
```

This will:
1. Load 654 movies from crawled database
2. Extract 25-35 test queries from test_datasets/
3. Automatically match entities to relevant documents
4. Evaluate both GraphRAG and SimpleRAG
5. Generate comparison reports

### Test Ground Truth Quality

```bash
python test_ground_truth_integration.py
```

Shows:
- Number of movies loaded
- Dataset parsing status
- Ground truth statistics
- Sample queries and their relevant documents

### Programmatic Usage

```python
from evaluate_mrr_map import (
    load_test_queries_with_relevance,
    RetrievalEvaluator
)

# Load queries with automatic ground truth
test_queries = load_test_queries_with_relevance()

# Create evaluator
evaluator = RetrievalEvaluator()

# Evaluate system
report = evaluator.evaluate_retrieval(test_queries, rag_type='graphrag')

# View results
evaluator.print_summary(report)
```

## Output Files

After running evaluation:

1. **mrr_map_evaluation_graphrag_YYYYMMDD_HHMMSS.json**
   - MRR, MAP@10, Recall@10, NDCG@10 scores
   - Per-query and aggregate metrics
   - Category breakdown

2. **mrr_map_evaluation_simplerage_YYYYMMDD_HHMMSS.json**
   - Same metrics for SimpleRAG system
   - Enables direct comparison

3. **mrr_map_comparison_YYYYMMDD_HHMMSS.json**
   - Side-by-side system comparison
   - Best system per metric
   - Overall statistics

### Example Report Structure

```json
{
  "rag_type": "graphrag",
  "timestamp": "2026-01-06T...",
  "query_results": [
    {
      "query_id": 1,
      "query": "Which actors have worked with Christopher Nolan...",
      "category": "actor_filmography",
      "relevant_count": 4,
      "mrr": 0.5,
      "map@10": 0.62,
      "recall@10": 0.75,
      "ndcg@10": 0.58,
      "latency_ms": 145.5,
      "success": true
    }
  ],
  "aggregate_metrics": {
    "total_queries": 25,
    "successful_queries": 24,
    "metrics": {
      "mrr": 0.65,
      "map@10": 0.71,
      "recall@10": 0.82,
      "ndcg@10": 0.68,
      "avg_latency_ms": 142.3
    },
    "category_breakdown": {
      "actor_filmography": {
        "count": 5,
        "mrr": 0.73,
        "map@10": 0.76,
        "recall@10": 0.85,
        "ndcg@10": 0.72
      }
    }
  }
}
```

## Metrics Explained

### MRR (Mean Reciprocal Rank)
Position of first relevant document:
- 1.0 = First result is relevant
- 0.5 = Second result is relevant
- 0.0 = No relevant results in top-10

### MAP@10 (Mean Average Precision)
Ranking quality across all relevant documents:
- Combines precision at each relevant rank
- Penalizes irrelevant results early in ranking
- Higher is better (0-1)

### Recall@10
Coverage of relevant documents in top-10:
- Ratio of retrieved relevant to all relevant
- Shows if relevant docs exist in ranking

### NDCG@10 (Normalized Discounted Cumulative Gain)
Position-weighted ranking quality:
- Accounts for position decay (better docs should rank higher)
- Normalized by ideal ranking
- Most comprehensive metric

## Next Steps

1. **Enhance Ground Truth**: Add more test queries manually
2. **Improve Matching**: Use semantic similarity instead of keyword matching
3. **Validate Results**: Cross-check with manual review
4. **Track Changes**: Re-run evaluation after system updates
5. **A/B Testing**: Compare GraphRAG vs SimpleRAG performance

## Integration Points

The evaluation system integrates with:

- ‚úÖ `src/rag_pipeline.py` (GraphRAG)
- ‚úÖ `src/simple_rag.py` (SimpleRAG)
- ‚úÖ `crawled_data/movies_index.json` (Movie database)
- ‚úÖ `test_datasets/*.json` (Test queries)
- ‚úÖ `test_dataset.json` (Main test set)

## Troubleshooting

### No relevant documents found for queries?

**Cause**: Entity-movie matching too strict
**Solution**: 
- Reduce case sensitivity (already done)
- Add substring matching (not just exact)
- Expand to genre keywords

```python
# Example: Be less strict
if entity_lower in title or  # Exact substring
   entity_lower[:5] in title:  # First 5 chars
    # Add to relevant
```

### Too many false positives?

**Cause**: Generic entity names (e.g., "hero", "villain")
**Solution**:
- Manual curation of important queries
- Add entity type hints
- Combine multiple entity matching strategies

### Movie database not loading?

**Cause**: Wrong path or JSON structure
**Solution**:
```bash
python -c "
import json
with open('crawled_data/movies_index.json') as f:
    data = json.load(f)
    print(f'Keys: {list(data.keys())}')
    print(f'Total movies: {len(data[\"movies\"])}')
"
```

## Performance Metrics

From initial test run:
- Ground truth extraction: ~15-20s for 25 queries
- Evaluation per query: ~1-2s (depends on retriever)
- Total evaluation time: ~2-3 minutes for full report

## References

- [RAGAS Evaluation](https://docs.ragas.io/)
- [BEIR Benchmark](https://arxiv.org/abs/2104.08663)
- [TREC Evaluation Metrics](https://trec.nist.gov/)
- [Information Retrieval Metrics](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))
