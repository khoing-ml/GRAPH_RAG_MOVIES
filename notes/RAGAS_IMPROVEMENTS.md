# Cáº£i tiáº¿n Manual RAGAS Metrics

## ðŸ“‹ Tá»•ng quan cáº£i tiáº¿n

File `manual_ragas_evaluation.py` Ä‘Ã£ Ä‘Æ°á»£c nÃ¢ng cáº¥p toÃ n diá»‡n vá»›i cÃ¡c cáº£i tiáº¿n vá» prompting, reliability vÃ  thÃªm metrics má»›i.

---

## ðŸŽ¯ Cáº£i tiáº¿n chÃ­nh

### 1. **Enhanced Prompting Strategy**

#### âœ… Chain-of-Thought Reasoning
- **TrÆ°á»›c**: LLM chá»‰ tráº£ vá» score Ä‘Æ¡n thuáº§n
- **Sau**: YÃªu cáº§u LLM giáº£i thÃ­ch reasoning trÆ°á»›c khi cháº¥m Ä‘iá»ƒm

**Format má»›i:**
```
REASONING: [Detailed analysis step-by-step]
SCORE: [0.0-1.0]
```

**Lá»£i Ã­ch:**
- âœ… Transparency - Hiá»ƒu Ä‘Æ°á»£c cÆ¡ sá»Ÿ Ä‘Ã¡nh giÃ¡
- âœ… Debugging - Dá»… phÃ¡t hiá»‡n lá»—i logic cá»§a LLM
- âœ… Consistency - Reasoning giÃºp scores á»•n Ä‘á»‹nh hÆ¡n

---

### 2. **Improved Error Handling & Retry Logic**

#### ðŸ”„ `_call_llm_with_retry()` Method
```python
def _call_llm_with_retry(self, prompt: str, max_retries: int = 3) -> tuple[float, str]:
    """
    Gá»i LLM vá»›i retry tá»± Ä‘á»™ng
    Returns: (score, reasoning_text)
    """
```

**Cáº£i tiáº¿n:**
- Retry tá»‘i Ä‘a 3 láº§n khi API fail
- Extract score tá»« nhiá»u format khÃ¡c nhau
- Fallback vá» 0.5 náº¿u hoÃ n toÃ n fail
- Tráº£ vá» cáº£ reasoning Ä‘á»ƒ debug

---

### 3. **Enhanced Metric Prompts**

#### ðŸ“Š Faithfulness (Improved)
**Cáº£i tiáº¿n:**
- âœ… Claim-by-claim analysis requirement
- âœ… Concrete examples cá»§a hallucination
- âœ… Detailed rubric (0.4, 0.6, 0.7, 0.8, 0.9, 1.0)
- âœ… Chain-of-thought instructions

**Scoring Rubric:**
```
1.0 = Perfect (100% claims supported)
0.9 = Excellent (>90% supported)
0.8 = Good (80-90% supported)
0.7 = Fair (70-80% supported)
0.6 = Mediocre (60-70% supported)
<0.6 = Poor (majority hallucinated)
```

---

#### ðŸŽ¯ Answer Relevancy (Improved)
**Multi-aspect evaluation:**
1. **DIRECTNESS** - Tráº£ lá»i Ä‘Ãºng cÃ¢u há»i?
2. **COMPLETENESS** - Äáº§y Ä‘á»§ thÃ´ng tin?
3. **FOCUS** - Táº­p trung, khÃ´ng lan man?

**Examples trong prompt:**
```
Question: "Who directed Inception?"
Answer: "Christopher Nolan" â†’ 1.0 (perfect)
Answer: "Christopher Nolan directed it in 2010..." â†’ 0.9 (verbose)
Answer: "It's a science fiction film..." â†’ 0.2 (irrelevant)
```

---

#### ðŸŽ¯ Context Precision (Improved)
**Context-by-context scoring:**
- ÄÃ¡nh giÃ¡ tá»«ng context riÃªng láº»
- PhÃ¢n loáº¡i: HIGHLY RELEVANT (1.0), SOMEWHAT RELEVANT (0.5), IRRELEVANT (0.0)
- TÃ­nh precision = average cá»§a cÃ¡c scores

**Example trong prompt:**
```
Question: "Who directed Avatar: Fire and Ash?"
Context 1: "Avatar: Fire and Ash directed by James Cameron" â†’ 1.0
Context 2: "Avatar (2009) also by Cameron" â†’ 0.5
Context 3: "Titanic won 11 Oscars" â†’ 0.0
Precision: (1.0 + 0.5 + 0.0) / 3 = 0.5
```

---

#### ðŸ” Context Recall (Improved)
**Information coverage analysis:**
- Liá»‡t kÃª key facts trong ground truth
- Check tá»«ng fact cÃ³ trong contexts khÃ´ng
- Calculate: facts_found / total_facts

**Example:**
```
Ground Truth: "Inception (2010) directed by Nolan, starring DiCaprio"
Key Facts: [title, year, director, actor]
Context covers 3/4 facts â†’ 0.75 recall
```

---

#### âœ… Answer Correctness (Improved)
**Two-dimensional evaluation:**
1. **FACTUAL ACCURACY** - Facts Ä‘Ãºng khÃ´ng?
2. **COMPLETENESS** - Äá»§ thÃ´ng tin chÆ°a?

**Semantic matching:**
- "directed by" = "director:" (tÆ°Æ¡ng Ä‘Æ°Æ¡ng)
- Extra correct info = OK (khÃ´ng trá»« Ä‘iá»ƒm)
- Wrong info = Severely penalized

---

### 4. **New Metrics Added** ðŸ†•

#### ðŸ“‹ Response Completeness
**Purpose:** Äo user satisfaction

**Criteria:**
- âœ… Provides all expected information
- âœ… Sufficient detail (not too brief/verbose)
- âœ… Actionable information
- âœ… No obvious questions left unanswered

**Example:**
```
Question: "Tell me about Inception"
"A sci-fi film" â†’ 0.2 (too brief)
"A 2010 sci-fi by Nolan about dreams" â†’ 0.7 (good)
"A 2010 sci-fi by Nolan starring DiCaprio about dream heists. Acclaimed." â†’ 1.0 (satisfying)
```

---

#### ðŸ”— Source Attribution
**Purpose:** Äo traceability cá»§a information

**Evaluation:**
- CÃ³ citation markers? ("according to", "based on", etc.)
- Information cÃ³ trace vá» contexts Ä‘Æ°á»£c khÃ´ng?
- User cÃ³ verify sources Ä‘Æ°á»£c khÃ´ng?

**Scoring:**
```
1.0 = Explicit citations for all claims
0.8 = Clear implicit attribution
0.6 = Partial attribution
0.4 = Weak attribution
0.2 = No attribution
```

**Note:** Low score = hard to verify, not wrong

---

### 5. **Weighted Overall Score**

#### âš–ï¸ Intelligent Weighting
**TrÆ°á»›c:** Simple average cá»§a táº¥t cáº£ metrics
**Sau:** Weighted average Æ°u tiÃªn metrics quan trá»ng

```python
weights = {
    'faithfulness': 1.5,          # Critical - no hallucination
    'answer_relevancy': 1.5,      # Critical - answers question
    'answer_correctness': 1.5,    # Critical - factually correct
    'context_precision': 1.0,     # Important - quality retrieval
    'context_recall': 1.0,        # Important - complete retrieval
    'response_completeness': 0.8, # Nice to have - satisfaction
    'source_attribution': 0.7     # Nice to have - traceability
}
```

**Output:**
- `overall_weighted`: Score cÃ³ trá»ng sá»‘ (primary metric)
- `overall_simple`: Simple average (for comparison)

---

### 6. **Better Context Formatting**

#### ðŸ“ Improved `_format_contexts()`
**TrÆ°á»›c:**
```
Context 1: Movie info...
Context 2: More info...
```

**Sau:**
```
--- CONTEXT 1 ---
Movie info with clear structure...
--- END CONTEXT 1 ---

--- CONTEXT 2 ---
More info...
--- END CONTEXT 2 ---

[... 3 more contexts not shown ...]
```

**Lá»£i Ã­ch:**
- âœ… RÃµ rÃ ng hÆ¡n cho LLM
- âœ… Dá»… reference "Context 2" trong evaluation
- âœ… Show truncation info

---

## ðŸ“Š Metrics Summary

| Metric | Type | Score Range | Weight | Purpose |
|--------|------|-------------|--------|---------|
| **Faithfulness** | Core | 0-1 | 1.5x | Prevent hallucination |
| **Answer Relevancy** | Core | 0-1 | 1.5x | Answer the question |
| **Answer Correctness** | Core | 0-1 | 1.5x | Factual accuracy |
| **Context Precision** | Retrieval | 0-1 | 1.0x | Low noise |
| **Context Recall** | Retrieval | 0-1 | 1.0x | Complete retrieval |
| **Response Completeness** | ðŸ†• UX | 0-1 | 0.8x | User satisfaction |
| **Source Attribution** | ðŸ†• Trust | 0-1 | 0.7x | Traceability |

**Total: 7 metrics** (5 original + 2 new)

---

## ðŸŽ¯ Usage Example

```python
from manual_ragas_evaluation import ManualRAGASEvaluator

evaluator = ManualRAGASEvaluator()
evaluator.debug_mode = True  # Show LLM reasoning

metrics = evaluator.evaluate_single(
    question="Who directed Inception?",
    answer="Christopher Nolan directed Inception, a 2010 sci-fi film.",
    contexts=[
        "Inception is a 2010 film directed by Christopher Nolan",
        "The film stars Leonardo DiCaprio"
    ],
    ground_truth="Christopher Nolan"
)

print(f"Faithfulness: {metrics['faithfulness']:.3f}")
print(f"Answer Relevancy: {metrics['answer_relevancy']:.3f}")
print(f"Response Completeness: {metrics['response_completeness']:.3f}")
print(f"Overall (weighted): {metrics['overall_weighted']:.3f}")
```

---

## ðŸ”„ Comparison: Before vs After

| Aspect | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Prompts** | Generic, brief | Detailed, with examples | ðŸ”¥ðŸ”¥ðŸ”¥ |
| **Error Handling** | Basic try-catch | Retry logic + fallbacks | ðŸ”¥ðŸ”¥ |
| **Transparency** | Score only | Score + reasoning | ðŸ”¥ðŸ”¥ðŸ”¥ |
| **Metrics** | 5 standard | 7 (+ 2 new) | ðŸ”¥ðŸ”¥ |
| **Scoring** | Simple average | Weighted + simple | ðŸ”¥ðŸ”¥ |
| **Context Format** | Basic | Structured | ðŸ”¥ |
| **Rubrics** | Vague (0.8, 0.6) | Precise (0.9, 0.8, 0.7) | ðŸ”¥ðŸ”¥ |

---

## ðŸ“ˆ Expected Impact

### Reliability
- âœ… More consistent scores across runs
- âœ… Fewer API failures with retry logic
- âœ… Better handling of edge cases

### Transparency
- âœ… Understand why scores are what they are
- âœ… Debug problematic evaluations
- âœ… Justify scores to stakeholders

### Coverage
- âœ… 2 new metrics cover UX and trust aspects
- âœ… More comprehensive RAG evaluation
- âœ… Better differentiate systems

### Accuracy
- âœ… More detailed prompts â†’ better LLM performance
- âœ… Examples in prompts â†’ calibrated scoring
- âœ… Weighted average â†’ prioritize critical metrics

---

## ðŸ› ï¸ Configuration

### Debug Mode
```python
evaluator.debug_mode = True   # Show reasoning (verbose)
evaluator.debug_mode = False  # Hide reasoning (clean output)
```

### Retry Settings
```python
# In _call_llm_with_retry()
max_retries = 3  # Adjust if needed
```

### Weights Customization
```python
# In evaluate_single()
weights = {
    'faithfulness': 2.0,  # Increase if hallucination is critical
    # ... customize as needed
}
```

---

## ðŸŽ“ Best Practices

1. **Debug Mode**: Always ON during development
2. **Weights**: Adjust based on your use case
3. **Ground Truth**: Provide when available (improves recall/correctness)
4. **Context Quality**: Better contexts â†’ better scores
5. **Sample Size**: Evaluate on 20+ queries for statistical validity

---

## ðŸ“ Notes

- **API Cost**: 7 LLM calls per evaluation (increased from 5)
- **Time**: ~10-15 seconds per evaluation (with retries)
- **Token Usage**: ~2000-3000 tokens per evaluation
- **Rate Limits**: Consider API limits when batch evaluating

---

## ðŸš€ Future Improvements

Potential enhancements:
- [ ] Multi-turn conversation support
- [ ] Aspect-based scoring (e.g., score different parts of answer)
- [ ] Confidence intervals for scores
- [ ] Automated weight tuning based on dataset
- [ ] Human-in-the-loop calibration

---

## ðŸ“š References

**RAGAS Framework:**
- Original paper: https://arxiv.org/abs/2309.15217
- Our implementation: Manual LLM-as-judge approach

**Improvements based on:**
- Chain-of-thought prompting research
- LLM evaluation best practices
- RAG system evaluation literature

---

**Last Updated**: January 6, 2026  
**Version**: 2.0 (Enhanced)
