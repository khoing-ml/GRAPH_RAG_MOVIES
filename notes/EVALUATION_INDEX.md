# MRR/MAP Evaluation Framework - Complete Index

## üìã Quick Navigation

### üöÄ Getting Started
- **[EVALUATION_COMPLETE_SUMMARY.md](EVALUATION_COMPLETE_SUMMARY.md)** - START HERE
  - Overview of what was built
  - Quick start guide
  - Key features and results

### üìö Documentation
1. **[MRR_MAP_EVALUATION.md](MRR_MAP_EVALUATION.md)**
   - Detailed metric explanations
   - Mathematical formulas
   - Usage examples
   - Interpretation guidelines

2. **[GROUND_TRUTH_INTEGRATION.md](GROUND_TRUTH_INTEGRATION.md)**
   - Ground truth data sources
   - Relevance extraction strategy
   - Data quality metrics
   - Integration points

3. **[EVALUATION_ARCHITECTURE.md](EVALUATION_ARCHITECTURE.md)**
   - System architecture
   - Data flow diagrams
   - Code organization
   - Performance analysis

### üíª Code Files

#### Main Evaluation
- **evaluate_mrr_map.py** (648 lines)
  - Core evaluation framework
  - RetrievalEvaluator class
  - MRR, MAP, Recall, NDCG calculations
  - Integration with GraphRAG/SimpleRAG

#### Testing & Validation
- **test_ground_truth_integration.py** (150 lines)
  - Fast ground truth validation
  - No RAG dependencies
  - Useful for debugging

- **test_evaluation_mock.py** (280 lines)
  - Mock evaluation without RAG
  - Tests 3 quality scenarios
  - Fast iteration

#### Shell Scripts
- **QUICKSTART_EVALUATION.sh**
  - Quick start guide for shell

## üéØ Main Metrics Explained

| Metric | Purpose | Range | Example |
|--------|---------|-------|---------|
| **MRR** | Position of 1st relevant | 0-1 | 0.798 |
| **MAP@10** | Ranking quality | 0-1 | 0.681 |
| **Recall@10** | Coverage | 0-1 | 0.895 |
| **NDCG@10** | Position-weighted quality | 0-1 | 0.793 |

## üìä Test Data

**30 test queries** from test_datasets/:
- Director filmography: 6 queries
- Actor filmography: 5 queries
- Multi-hop reasoning: 5 queries
- Comparison: 5 queries
- Temporal analysis: 5 queries
- Genre recommendation: 2 queries
- Others: 2 queries

**Ground Truth Quality:**
- 19/30 queries (63%) have relevant documents
- 126 total relevant documents
- Average 4.20 relevant per query

## üöÄ Quick Commands

### Test Ground Truth
```bash
python test_ground_truth_integration.py
```

### Mock Evaluation (Fast)
```bash
python test_evaluation_mock.py
```

### Full Evaluation (Slow)
```bash
python evaluate_mrr_map.py
```

## üìÅ Related Files

**Movie Database:**
- crawled_data/movies_index.json (654 movies)

**Test Datasets:**
- test_datasets/actor_based.json
- test_datasets/director_based.json
- test_datasets/multi_hop.json
- test_datasets/comparison.json
- test_datasets/temporal_based.json
- test_datasets/genre_recommendation.json
- test_datasets/specific_film_info.json

**RAG Integration:**
- src/rag_pipeline.py (GraphRAG)
- src/simple_rag.py (SimpleRAG)

## üí° Use Cases

### 1. Evaluate New Retriever
```python
from evaluate_mrr_map import load_test_queries_with_relevance, RetrievalEvaluator

queries = load_test_queries_with_relevance()
evaluator = RetrievalEvaluator()
report = evaluator.evaluate_retrieval(queries, rag_type='graphrag')
evaluator.print_summary(report)
```

### 2. Compare Two Systems
```python
graphrag_report = evaluator.evaluate_retrieval(queries, 'graphrag')
simplerage_report = evaluator.evaluate_retrieval(queries, 'simplerage')
comparison = evaluator.compare_systems([graphrag_report, simplerage_report])
```

### 3. Fast Testing with Mock
```python
from test_evaluation_mock import run_mock_evaluation

report = run_mock_evaluation(queries, noise_ratio=0.3)
# Results without waiting for RAG
```

## üìà Evaluation Results

### High Quality Scenario (80% relevant)
- MRR: 0.798
- MAP: 0.681
- Recall: 0.895
- NDCG: 0.793

### Medium Quality (50% relevant)
- MRR: 0.655
- MAP: 0.396
- Recall: 0.737
- NDCG: 0.579

### Low Quality (20% relevant)
- MRR: 0.355
- MAP: 0.080
- Recall: 0.286
- NDCG: 0.185

## ‚ùì FAQ

**Q: Why only 63% of queries have relevant docs?**
A: Automatic entity matching is strict to avoid false positives. Manual curation can improve this.

**Q: How long does evaluation take?**
A: Mock: ~13s. Full: ~30-60 minutes (depends on RAG complexity).

**Q: Which metric should I use?**
A: Start with MRR for quick assessment, use MAP/NDCG for comprehensive evaluation.

**Q: Can I add custom test queries?**
A: Yes! Modify `load_test_queries_with_relevance()` in evaluate_mrr_map.py

**Q: How do I improve ground truth quality?**
A: See GROUND_TRUTH_INTEGRATION.md for strategies.

## üîó Related Documentation

- [RAGAS Framework](https://docs.ragas.io/)
- [BEIR Benchmark](https://arxiv.org/abs/2104.08663)
- [TREC Evaluation](https://trec.nist.gov/)
- [IR Metrics](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))

## üìù Notes

- All scripts tested and working ‚úÖ
- Mock evaluation validates framework ‚úÖ
- Full evaluation ready but time-intensive ‚è±Ô∏è
- Integration with RAG systems working ‚úÖ

## üéØ Next Steps

1. Choose evaluation type (mock vs full)
2. Run appropriate evaluation script
3. Review metrics and category breakdown
4. Compare results over time
5. Iterate on retriever improvements

---

**Last Updated:** January 6, 2026
**Status:** Production Ready ‚úÖ
**Files:** 3 Python scripts + 4 Documentation files
