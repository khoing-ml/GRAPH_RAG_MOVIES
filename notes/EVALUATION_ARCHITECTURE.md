# MRR/MAP Evaluation Integration Architecture

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MRR/MAP Evaluation System                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Ground Truth Integration Layer                  â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â”‚  ğŸ“‚ test_datasets/                                         â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ actor_based.json      (5 queries)               â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ director_based.json   (5 queries)               â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ multi_hop.json        (5 queries)               â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ comparison.json       (5 queries)               â”‚   â”‚
â”‚  â”‚     â””â”€â”€ temporal_based.json   (5 queries)               â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â”‚  ğŸ“„ test_dataset.json         (1000+ queries)            â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      Relevance Matching Engine                            â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â”‚  1. Extract query entities                                â”‚   â”‚
â”‚  â”‚  2. Load 654 movies from crawled database                â”‚   â”‚
â”‚  â”‚  3. Match entities to movie metadata:                    â”‚   â”‚
â”‚  â”‚     â€¢ Title matching                                      â”‚   â”‚
â”‚  â”‚     â€¢ Director matching                                   â”‚   â”‚
â”‚  â”‚     â€¢ Cast matching                                       â”‚   â”‚
â”‚  â”‚     â€¢ Genre matching                                      â”‚   â”‚
â”‚  â”‚     â€¢ Overview matching                                   â”‚   â”‚
â”‚  â”‚  4. Build ground truth (relevant_doc_ids)                â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Evaluation Engine (RetrievalEvaluator)            â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â”‚  For each test query:                                      â”‚   â”‚
â”‚  â”‚  1. Retrieve documents (k=10)                            â”‚   â”‚
â”‚  â”‚  2. Calculate MRR                                         â”‚   â”‚
â”‚  â”‚  3. Calculate MAP@10                                      â”‚   â”‚
â”‚  â”‚  4. Calculate Recall@10                                   â”‚   â”‚
â”‚  â”‚  5. Calculate NDCG@10                                     â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â”‚  Compare:                                                  â”‚   â”‚
â”‚  â”‚  â€¢ GraphRAG vs SimpleRAG                                  â”‚   â”‚
â”‚  â”‚  â€¢ By category (actor, director, multi_hop, etc)         â”‚   â”‚
â”‚  â”‚  â€¢ Aggregate statistics                                    â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           JSON Report Generation                           â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â”‚  ğŸ“„ mrr_map_evaluation_graphrag_YYYYMMDD_HHMMSS.json      â”‚   â”‚
â”‚  â”‚  ğŸ“„ mrr_map_evaluation_simplerage_YYYYMMDD_HHMMSS.json    â”‚   â”‚
â”‚  â”‚  ğŸ“„ mrr_map_comparison_YYYYMMDD_HHMMSS.json               â”‚   â”‚
â”‚  â”‚                                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

### 1. Ground Truth Extraction

```python
Query: "Which actors have worked with Christopher Nolan?"
Entities: ["Christopher Nolan"]

â†“ (Relevance Matching)

Movie Database Scan (654 movies):
  â€¢ Inception â†’ Director: "Christopher Nolan" âœ“
  â€¢ The Dark Knight â†’ Director: "Christopher Nolan" âœ“
  â€¢ Interstellar â†’ Director: "Christopher Nolan" âœ“
  â€¢ Oppenheimer â†’ Director: "Christopher Nolan" âœ“
  â€¢ Random movie â†’ Director: "Other" âœ—

â†“

Ground Truth Output:
{
  "relevant_doc_ids": ["inception", "the_dark_knight", "interstellar", "oppenheimer"],
  "count": 4
}
```

### 2. Evaluation Flow

```python
Test Query: {
  "query": "Which actors have worked with Christopher Nolan?",
  "relevant_doc_ids": ["inception", "the_dark_knight", "interstellar", "oppenheimer"],
  "category": "actor_filmography"
}

â†“ (Retrieve)

RAG System (GraphRAG):
  retrieve(query, k=10)
  â†’ [
      {"id": "inception", "title": "Inception", ...},
      {"id": "other_movie", ...},
      {"id": "the_dark_knight", ...},
      ...
    ]

â†“ (Evaluate)

Metrics Calculation:
  MRR = 1/1 = 1.0 (first result is relevant)
  MAP@10 = (1.0 + 0.67) / 4 = 0.42 (positions 1, 3 relevant)
  Recall@10 = 2/4 = 0.5 (found 2 of 4 relevant docs)
  NDCG@10 = 0.65 (position-weighted)

â†“

Report Entry:
{
  "query_id": 1,
  "query": "Which actors have worked with Christopher Nolan?",
  "mrr": 1.0,
  "map@10": 0.42,
  "recall@10": 0.5,
  "ndcg@10": 0.65,
  "success": true
}
```

## Code Organization

### Files Created

```
evaluate_mrr_map.py (642 lines)
â”œâ”€â”€ load_movie_database()
â”œâ”€â”€ load_test_queries_from_datasets()
â”œâ”€â”€ load_test_queries_with_relevance()
â”œâ”€â”€ RetrievalEvaluator class
â”‚   â”œâ”€â”€ calculate_mrr()
â”‚   â”œâ”€â”€ calculate_map()
â”‚   â”œâ”€â”€ calculate_recall_at_k()
â”‚   â”œâ”€â”€ calculate_ndcg()
â”‚   â”œâ”€â”€ evaluate_retrieval()
â”‚   â”œâ”€â”€ print_summary()
â”‚   â”œâ”€â”€ compare_systems()
â”‚   â””â”€â”€ save_report()
â””â”€â”€ main()

test_ground_truth_integration.py (standalone test utility)
â”œâ”€â”€ load_movie_database()
â”œâ”€â”€ load_test_queries_from_datasets()
â”œâ”€â”€ analyze_ground_truth()
â””â”€â”€ main()
```

### Documentation Files

```
MRR_MAP_EVALUATION.md
â”œâ”€â”€ Metric explanations with formulas
â”œâ”€â”€ Usage examples
â”œâ”€â”€ Output format
â””â”€â”€ Troubleshooting

GROUND_TRUTH_INTEGRATION.md
â”œâ”€â”€ Integration summary
â”œâ”€â”€ Data sources
â”œâ”€â”€ Ground truth quality metrics
â”œâ”€â”€ Relevance matching strategy
â””â”€â”€ Next steps

QUICKSTART_EVALUATION.sh
â””â”€â”€ Quick start guide
```

## Database Structure

### Movie Database (654 movies)

```json
{
  "total_movies": 654,
  "crawl_date": "2026-01-05",
  "movies": {
    "inception": {
      "id": "inception",
      "title": "Inception",
      "release_date": "2010-07-16",
      "genres": ["Sci-Fi", "Thriller", "Action"],
      "director": "Christopher Nolan",
      "cast": ["Leonardo DiCaprio", "Marion Cotillard", ...],
      "overview": "A skilled thief who steals corporate secrets...",
      "rating": 8.8
    },
    ...
  }
}
```

## Test Query Structure

### From test_datasets/*.json

```json
{
  "category": "actor_filmography",
  "description": "Complex queries about actors...",
  "test_cases": [
    {
      "id": 1,
      "query": "Which actors have worked with Christopher Nolan multiple times?",
      "entities": ["Christopher Nolan"],
      "relations": ["ACTED_IN", "DIRECTED_BY"],
      "complexity": "high"
    }
  ]
}
```

### Processed Format (After Ground Truth Extraction)

```json
{
  "id": 1,
  "query": "Which actors have worked with Christopher Nolan multiple times?",
  "category": "actor_filmography",
  "relevant_doc_ids": ["inception", "the_dark_knight", "interstellar", "oppenheimer"],
  "entities": ["Christopher Nolan"],
  "complexity": "high",
  "expected_answer": ""
}
```

## Evaluation Report Structure

### Individual Query Result

```json
{
  "query_id": 1,
  "query": "Which actors have worked with Christopher Nolan...",
  "category": "actor_filmography",
  "relevant_count": 4,
  "retrieved_count": 10,
  "mrr": 0.75,
  "map@10": 0.42,
  "recall@10": 0.5,
  "ndcg@10": 0.58,
  "latency_ms": 145.5,
  "success": true
}
```

### Aggregate Metrics

```json
{
  "aggregate_metrics": {
    "total_queries": 25,
    "successful_queries": 24,
    "failed_queries": 1,
    "metrics": {
      "mrr": 0.65,
      "map@10": 0.71,
      "recall@10": 0.82,
      "ndcg@10": 0.68,
      "avg_latency_ms": 142.3
    },
    "category_breakdown": {
      "actor_filmography": {
        "count": 5,
        "mrr": 0.73,
        "map@10": 0.76,
        "recall@10": 0.85,
        "ndcg@10": 0.72
      }
    }
  }
}
```

## Integration with RAG Systems

### GraphRAG Integration

```python
from src.rag_pipeline import GraphRAG

rag = GraphRAG()
retrieved_docs = rag.retrieve(query, k=10)
# Expected format: List[Dict] with 'id', 'title', 'score' keys
```

### SimpleRAG Integration

```python
from src.simple_rag import SimpleRAG

rag = SimpleRAG()
retrieved_docs = rag.retrieve(query, k=10)
# Same format as GraphRAG
```

## Execution Workflow

```
1. Load Movie Database (654 movies)
   â†“
2. Load Test Datasets (25 queries)
   â†“
3. Extract Ground Truth (match entities to movies)
   â†“
4. Evaluate GraphRAG
   â”œâ”€ Retrieve documents for each query
   â”œâ”€ Calculate metrics (MRR, MAP, Recall, NDCG)
   â”œâ”€ Aggregate results
   â””â”€ Generate report
   â†“
5. Evaluate SimpleRAG
   â”œâ”€ Same as GraphRAG
   â””â”€ Generate report
   â†“
6. Compare Systems
   â”œâ”€ Side-by-side metrics
   â”œâ”€ Best system per metric
   â””â”€ Generate comparison report
   â†“
7. Output JSON Reports
   â”œâ”€ mrr_map_evaluation_graphrag_*.json
   â”œâ”€ mrr_map_evaluation_simplerage_*.json
   â””â”€ mrr_map_comparison_*.json
```

## Performance Characteristics

### Time Complexity

```
Ground Truth Extraction:
  O(T Ã— M) where:
    T = number of test queries = 25
    M = number of movies = 654
  â†’ ~16,350 comparisons

Evaluation Per Query:
  O(K Ã— R) where:
    K = top-k results = 10
    R = relevant documents = avg 3.84
  â†’ ~38 comparisons per query
  â†’ ~950 total for 25 queries

Total:
  Ground truth: ~1-2 seconds
  Evaluation: ~3-5 seconds (depends on retriever)
  Report generation: <1 second
  â†’ ~4-8 seconds total
```

### Space Complexity

```
Movie Database: ~1.5 MB (654 movies in memory)
Test Queries: ~50 KB (25 queries)
Report Output: ~100 KB per report
```

## Quality Metrics

### Current Ground Truth Quality

```
âœ… Coverage: 60% of queries have relevant documents
âœ… Relevance: 3.84 avg relevant docs per query
âœ… Balance: Evenly distributed across categories
âœ… Complexity: Mix of simple to very_high complexity
```

### Evaluation Metrics Properties

```
MRR:   Ranges [0, 1], captures first-result quality
MAP:   Ranges [0, 1], comprehensive ranking quality
Recall: Ranges [0, 1], shows coverage
NDCG:  Ranges [0, 1], position-aware ranking quality
```

## Future Enhancements

1. **Semantic Matching**: Use embeddings instead of keyword matching
2. **Manual Curation**: Add hand-picked ground truth for edge cases
3. **Relevance Grading**: Use 0/1/2 grades instead of binary
4. **More Queries**: Expand from 25 to 100+ test queries
5. **Cross-validation**: k-fold evaluation
6. **Statistical Testing**: Significance tests between systems
