import streamlit as st
from src.rag_pipeline import GraphRAG
import time
from streamlit.runtime.scriptrunner import get_script_run_ctx

# H√†m kh·ªüi t·∫°o v√† l∆∞u tr·ªØ GraphRAG v√†o session state
@st.cache_resource
def initialize_graph_rag():
    """Kh·ªüi t·∫°o GraphRAG service v√† cache n√≥."""
    try:
        rag_service = GraphRAG()
        return rag_service
    except Exception as e:
        # N·∫øu database ch∆∞a b·∫≠t ho·∫∑c key sai
        st.error(f"‚ùå L·ªói kh·ªüi t·∫°o h·ªá th·ªëng: {e}")
        st.stop()

# --- C·∫§U H√åNH GIAO DI·ªÜN STREAMLIT ---
st.set_page_config(
    page_title="Book GraphRAG Advisor",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("üìö Book Recommender (GraphRAG + Gemini)")
st.subheader("T√¨m ki·∫øm ng·ªØ nghƒ©a v√† quan h·ªá gi·ªØa s√°ch")

# Kh·ªüi t·∫°o d·ªãch v·ª• ch·ªâ m·ªôt l·∫ßn
rag = initialize_graph_rag()

# --- X·ª≠ l√Ω L·ªãch s·ª≠ Chat ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({
        "role": "assistant", 
        "content": "Ch√†o b·∫°n! B·∫°n mu·ªën t√¨m s√°ch v·ªÅ ch·ªß ƒë·ªÅ g√¨, ho·∫∑c mu·ªën t√¨m s√°ch c√πng t√°c gi·∫£ n√†o?"
    })

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- X·ª≠ l√Ω Input c·ªßa Ng∆∞·ªùi d√πng ---
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # 1. Th√™m c√¢u h·ªèi ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. Sinh c√¢u tr·∫£ l·ªùi v√† hi·ªÉn th·ªã
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # G·ªçi h√†m query t·ª´ service GraphRAG
        try:
            with st.spinner("ü§ñ ƒêang suy lu·∫≠n b·∫±ng GraphRAG..."):
                start_time = time.time()
                
                # H√†m query c·ªßa ch√∫ng ta ƒë√£ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ tr·∫£ v·ªÅ chu·ªói cu·ªëi c√πng
                ai_response = rag.query(prompt)
                
                end_time = time.time()
                latency = end_time - start_time

                # Hi·ªÉn th·ªã k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng stream (gi·∫£ l·∫≠p)
                # Ho·∫∑c ch·ªâ hi·ªÉn th·ªã m·ªôt l·∫ßn n·∫øu Gemini tr·∫£ v·ªÅ nhanh
                full_response = ai_response + f"\n\n---\n*Ph·∫£n h·ªìi trong: {latency:.2f}s*"
                
                message_placeholder.markdown(full_response)

        except Exception as e:
            full_response = f"‚ùå Xin l·ªói, c√≥ l·ªói h·ªá th·ªëng x·∫£y ra: {e}"
            message_placeholder.markdown(full_response)

    # 3. L∆∞u c√¢u tr·∫£ l·ªùi c·ªßa tr·ª£ l√Ω v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "assistant", "content": full_response})

# --- Sidebar Th√¥ng tin ---
with st.sidebar:
    st.header("Th√¥ng tin D·ª± √°n")
    st.write("Ki·∫øn tr√∫c: Hybrid RAG (Retrieval-Augmented Generation)")
    st.write(f"LLM: Gemini-2.5-Flash (via `src/llm_service.py`)")
    st.write(f"Vector DB: Qdrant (C·ªïng 6333)")
    st.write(f"Graph DB: Neo4j (C·ªïng 7687)")
    
    st.button("X√≥a L·ªãch s·ª≠ Chat", on_click=lambda: st.session_state.messages.clear())